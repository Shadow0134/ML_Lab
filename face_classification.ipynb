{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuXN-8wCQMiD",
        "outputId": "8a89f3e4-24d4-45d1-9246-cea73c3a1554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.10)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.2.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch and other packages (in Colab environment)\n",
        "!pip install torch torchvision albumentations opencv-python tqdm\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fuxzg2D6QV_B",
        "outputId": "22f5c097-5031-4008-ca8e-4905edb002f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading WIDER FACE dataset from Kaggle...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/iamprateek/wider-face-a-face-detection-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.43G/3.43G [00:38<00:00, 94.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original KaggleHub path: /root/.cache/kagglehub/datasets/iamprateek/wider-face-a-face-detection-dataset/versions/1\n",
            " Dataset copied to: /content/WIDERFace\n",
            "\n",
            " Detected paths:\n",
            "Train Images: /content/WIDERFace/WIDER_train/WIDER_train/images\n",
            "Val Images:   /content/WIDERFace/WIDER_val/WIDER_val/images\n",
            "Annotations: /content/WIDERFace/wider_face_annotations/wider_face_split\n",
            "\n",
            " Loaded 12880 training images\n",
            " Loaded 3226 validation images\n",
            "Example image path: /content/WIDERFace/WIDER_train/WIDER_train/images/0--Parade/0_Parade_marchingband_1_1038.jpg\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Install and download dataset via KaggleHub\n",
        "# ============================================================\n",
        "!pip install -q kagglehub\n",
        "\n",
        "import kagglehub, os, shutil, numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "print(\"Downloading WIDER FACE dataset from Kaggle...\")\n",
        "kaggle_path = kagglehub.dataset_download(\"iamprateek/wider-face-a-face-detection-dataset\")\n",
        "print(f\" Original KaggleHub path: {kaggle_path}\")\n",
        "\n",
        "# ============================================================\n",
        "# Move dataset into /content for easy access\n",
        "# ============================================================\n",
        "target_dir = \"/content/WIDERFace\"\n",
        "if os.path.exists(target_dir):\n",
        "    shutil.rmtree(target_dir)\n",
        "shutil.copytree(kaggle_path, target_dir)\n",
        "print(f\" Dataset copied to: {target_dir}\")\n",
        "\n",
        "# ============================================================\n",
        "# Auto-detect nested folder structure\n",
        "# ============================================================\n",
        "# Detect actual image and annotation locations\n",
        "train_dir = None\n",
        "val_dir = None\n",
        "ann_dir = None\n",
        "\n",
        "for root, dirs, files in os.walk(target_dir):\n",
        "    if \"WIDER_train\" in root and \"images\" in dirs:\n",
        "        train_dir = os.path.join(root, \"images\")\n",
        "    elif \"WIDER_val\" in root and \"images\" in dirs:\n",
        "        val_dir = os.path.join(root, \"images\")\n",
        "    elif \"wider_face_annotations\" in root:\n",
        "        ann_dir = root\n",
        "\n",
        "print(\"\\n Detected paths:\")\n",
        "print(\"Train Images:\", train_dir)\n",
        "print(\"Val Images:  \", val_dir)\n",
        "print(\"Annotations:\", ann_dir)\n",
        "\n",
        "# ============================================================\n",
        "#  Safe annotation loader\n",
        "# ============================================================\n",
        "def load_wider_annotations(split='train', ann_root=ann_dir):\n",
        "    \"\"\"Load WIDER FACE annotations safely from Kaggle dataset.\"\"\"\n",
        "    ann_file = os.path.join(ann_root, f\"wider_face_{split}_bbx_gt.txt\")\n",
        "    if not os.path.exists(ann_file):\n",
        "        print(f\" Annotation file not found: {ann_file}\")\n",
        "        return []\n",
        "\n",
        "    lines = open(ann_file).read().strip().splitlines()\n",
        "    idx = 0\n",
        "    annotations = []\n",
        "    while idx < len(lines):\n",
        "        img_path = lines[idx].strip()\n",
        "        idx += 1\n",
        "        try:\n",
        "            num_boxes = int(lines[idx].strip())\n",
        "            idx += 1\n",
        "        except ValueError:\n",
        "            while idx < len(lines) and not lines[idx].strip().endswith('.jpg'):\n",
        "                idx += 1\n",
        "            continue\n",
        "\n",
        "        boxes = []\n",
        "        for _ in range(num_boxes):\n",
        "            if idx >= len(lines): break\n",
        "            vals = list(map(int, lines[idx].split()))\n",
        "            if len(vals) >= 4 and vals[2] > 0 and vals[3] > 0:\n",
        "                boxes.append([vals[0], vals[1], vals[2], vals[3]])\n",
        "            idx += 1\n",
        "        annotations.append((img_path, np.array(boxes, dtype=np.float32)))\n",
        "    return annotations\n",
        "\n",
        "# ============================================================\n",
        "# Load annotations and verify\n",
        "# ============================================================\n",
        "train_annotations = load_wider_annotations('train', ann_root=ann_dir)\n",
        "val_annotations   = load_wider_annotations('val', ann_root=ann_dir)\n",
        "\n",
        "print(f\"\\n Loaded {len(train_annotations)} training images\")\n",
        "print(f\" Loaded {len(val_annotations)} validation images\")\n",
        "\n",
        "# ============================================================\n",
        "# Test a random image\n",
        "# ============================================================\n",
        "# The dataset stores paths like: 0--Parade/0_Parade_marchingband_1_1038.jpg\n",
        "# so we just prepend our detected train_dir to access the actual file.\n",
        "if len(train_annotations) > 0:\n",
        "    sample_img, _ = train_annotations[100]\n",
        "    full_path = os.path.join(train_dir, sample_img)\n",
        "    print(f\"Example image path: {full_path}\")\n",
        "    Image.open(full_path).show()\n",
        "else:\n",
        "    print(\"No training annotations loaded. Check annotation directory structure.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lPxdlo4BRSlP"
      },
      "outputs": [],
      "source": [
        "# Define transformations (resize and conversion to tensor)\n",
        "input_size = 300\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "def preprocess(image, boxes):\n",
        "    \"\"\"\n",
        "    Safely resize and normalize image, scaling boxes to input size.\n",
        "    Handles empty or malformed annotations robustly.\n",
        "    \"\"\"\n",
        "    orig_w, orig_h = image.size\n",
        "    image = image.resize((input_size, input_size))\n",
        "    scale_x = input_size / orig_w\n",
        "    scale_y = input_size / orig_h\n",
        "\n",
        "    # ---- Fix starts here ----\n",
        "    # Normalize malformed box arrays\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        boxes = np.zeros((0, 4), dtype=np.float32)\n",
        "    boxes = np.array(boxes, dtype=np.float32)\n",
        "\n",
        "    # If 1D (like [x,y,w,h]) reshape into (1,4)\n",
        "    if boxes.ndim == 1:\n",
        "        if boxes.size == 4:\n",
        "            boxes = boxes.reshape(1, 4)\n",
        "        else:\n",
        "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
        "\n",
        "    # Filter invalid or zero-sized boxes\n",
        "    valid = (boxes[:, 2] > 0) & (boxes[:, 3] > 0)\n",
        "    boxes = boxes[valid]\n",
        "    if boxes.size == 0:\n",
        "        boxes = np.zeros((0, 4), dtype=np.float32)\n",
        "    # ---- Fix ends here ----\n",
        "\n",
        "    # Convert (x, y, w, h) â†’ (xmin, ymin, xmax, ymax)\n",
        "    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
        "    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
        "\n",
        "    # Scale coordinates\n",
        "    boxes[:, 0] *= scale_x\n",
        "    boxes[:, 1] *= scale_y\n",
        "    boxes[:, 2] *= scale_x\n",
        "    boxes[:, 3] *= scale_y\n",
        "\n",
        "    # Random horizontal flip (data augmentation)\n",
        "    if random.random() < 0.5 and len(boxes) > 0:\n",
        "        image = transforms.functional.hflip(image)\n",
        "        boxes[:, [0, 2]] = input_size - boxes[:, [2, 0]]\n",
        "\n",
        "    # Convert to tensor format\n",
        "    img_tensor = to_tensor(image)\n",
        "    target = {\n",
        "        'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
        "        'labels': torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
        "    }\n",
        "    return img_tensor, target\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rPlPItxMRTn1"
      },
      "outputs": [],
      "source": [
        "class WIDERFaceDataset(Dataset):\n",
        "    def __init__(self, annotations, images_root, transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.images_root = images_root\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, boxes = self.annotations[idx]\n",
        "        img = Image.open(os.path.join(self.images_root, img_path)).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img_tensor, target = self.transform(img, boxes)\n",
        "        else:\n",
        "            img_tensor, target = to_tensor(img), {'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
        "                                                  'labels': torch.ones((len(boxes),), dtype=torch.int64)}\n",
        "        return img_tensor, target\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = WIDERFaceDataset(train_annotations, images_root=\"/content/WIDERFace/WIDER_train/WIDER_train/images\", transform=preprocess)\n",
        "val_dataset   = WIDERFaceDataset(val_annotations,   images_root=\"/content/WIDERFace/WIDER_val/WIDER_val/images\",   transform=preprocess)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda b: tuple(zip(*b)))\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=8, shuffle=False, collate_fn=lambda b: tuple(zip(*b)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bw_-0nJrRVvo"
      },
      "outputs": [],
      "source": [
        "class FaceDetectorSSD(nn.Module):\n",
        "    def __init__(self, num_anchors=4):\n",
        "        super().__init__()\n",
        "        # Define a small backbone with depthwise separable convs\n",
        "        def conv_bn_relu(in_ch, out_ch, stride=1):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True)\n",
        "            )\n",
        "        # Depthwise separable conv block\n",
        "        class DWBlock(nn.Module):\n",
        "            def __init__(self, in_ch, out_ch, stride=1):\n",
        "                super().__init__()\n",
        "                self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=stride, padding=1, groups=in_ch, bias=False)\n",
        "                self.bn1 = nn.BatchNorm2d(in_ch)\n",
        "                self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "                self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "                self.act = nn.ReLU(inplace=True)\n",
        "            def forward(self, x):\n",
        "                x = self.act(self.bn1(self.dw(x)))\n",
        "                x = self.act(self.bn2(self.pw(x)))\n",
        "                return x\n",
        "\n",
        "        # Backbone layers\n",
        "        self.layer1 = conv_bn_relu(3,  32, stride=2)  # 300x300 -> 150x150\n",
        "        self.layer2 = DWBlock(32, 32)                  # 150x150\n",
        "        self.layer3 = conv_bn_relu(32, 64, stride=2)  # 150x150 -> 75x75\n",
        "        self.layer4 = DWBlock(64, 64)                  # 75x75\n",
        "        self.layer5 = conv_bn_relu(64,128, stride=2)  # 75x75 -> 38x38\n",
        "        self.layer6 = DWBlock(128,128)                 # 38x38 final feature map\n",
        "\n",
        "        # Detection head: classification and regression\n",
        "        self.cls_head = nn.Conv2d(128, num_anchors,   3, padding=1)  # one score per anchor\n",
        "        self.reg_head = nn.Conv2d(128, num_anchors*4, 3, padding=1)  # 4 coords per anchor\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Backbone forward\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.layer6(x)  # final feature map (N,128,H,W)\n",
        "\n",
        "        # Predict scores and box offsets\n",
        "        cls_out = self.cls_head(x)  # (N, num_anchors, H, W)\n",
        "        reg_out = self.reg_head(x)  # (N, num_anchors*4, H, W)\n",
        "\n",
        "        # Rearrange to shape (N, H*W*num_anchors, 1) and (N, H*W*num_anchors, 4)\n",
        "        N, A, H, W = cls_out.shape\n",
        "        cls_out = cls_out.permute(0,2,3,1).contiguous().view(N, -1)  # flatten scores\n",
        "        reg_out = reg_out.view(N, A, 4, H, W).permute(0,3,4,1,2).contiguous().view(N, -1, 4)\n",
        "        return cls_out, reg_out\n",
        "\n",
        "# Instantiate the model\n",
        "num_anchors = 4  # e.g. 4 default boxes per cell\n",
        "model = FaceDetectorSSD(num_anchors=num_anchors).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F40gvn7dRbDO",
        "outputId": "118ace9a-29f9-4b2e-f8f1-26712424e576"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Generated 21660 anchors (5 scales Ã— 3 ratios)\n",
            "âœ… Model initialized: 15 anchors/cell Ã— 38Ã—38 = 21660 total\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:23<00:00,  4.98it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 1/50 | Avg Loss: 0.9138\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.9138)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:02<00:00,  5.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 2/50 | Avg Loss: 0.6796\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.6796)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:02<00:00,  5.33it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 3/50 | Avg Loss: 0.6452\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.6452)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:02<00:00,  5.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 4/50 | Avg Loss: 0.6224\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.6224)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:03<00:00,  5.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 5/50 | Avg Loss: 0.6051\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.6051)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:03<00:00,  5.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 6/50 | Avg Loss: 0.5937\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5937)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:04<00:00,  5.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 7/50 | Avg Loss: 0.5763\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5763)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:03<00:00,  5.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 8/50 | Avg Loss: 0.5679\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5679)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:03<00:00,  5.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 9/50 | Avg Loss: 0.5632\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5632)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 10/50 | Avg Loss: 0.5533\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5533)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 11/50 | Avg Loss: 0.5479\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5479)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:01<00:00,  5.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 12/50 | Avg Loss: 0.5427\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5427)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 13/50 | Avg Loss: 0.5368\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5368)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 14/50 | Avg Loss: 0.5310\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5310)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:01<00:00,  5.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 15/50 | Avg Loss: 0.5253\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5253)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.37it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 16/50 | Avg Loss: 0.5172\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5172)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 17/50 | Avg Loss: 0.5134\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5134)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.36it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Epoch 18/50 | Avg Loss: 0.5080\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5080)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:03<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 19/50 | Avg Loss: 0.5090\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 20/50 | Avg Loss: 0.5037\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.5037)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 21/50 | Avg Loss: 0.4996\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4996)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 22/50 | Avg Loss: 0.4982\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4982)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 23/50 | Avg Loss: 0.4959\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4959)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 24/50 | Avg Loss: 0.4938\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4938)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 25/50 | Avg Loss: 0.4900\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4900)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 26/50 | Avg Loss: 0.4877\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4877)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:01<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 27/50 | Avg Loss: 0.4861\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4861)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:01<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 28/50 | Avg Loss: 0.4838\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4838)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:02<00:00,  5.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 29/50 | Avg Loss: 0.4824\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4824)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:01<00:00,  5.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 30/50 | Avg Loss: 0.4781\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4781)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:01<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 31/50 | Avg Loss: 0.4772\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4772)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:58<00:00,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 32/50 | Avg Loss: 0.4764\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4764)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:58<00:00,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 33/50 | Avg Loss: 0.4706\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4706)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:01<00:00,  5.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 34/50 | Avg Loss: 0.4725\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 35/50 | Avg Loss: 0.4684\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4684)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 36/50 | Avg Loss: 0.4672\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4672)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:58<00:00,  5.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 37/50 | Avg Loss: 0.4652\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4652)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:03<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 38/50 | Avg Loss: 0.4644\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4644)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 39/50 | Avg Loss: 0.4626\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4626)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 40/50 | Avg Loss: 0.4599\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4599)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 41/50 | Avg Loss: 0.4592\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4592)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:03<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 42/50 | Avg Loss: 0.4576\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4576)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [05:00<00:00,  5.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 43/50 | Avg Loss: 0.4584\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:57<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 44/50 | Avg Loss: 0.4555\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4555)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:55<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 45/50 | Avg Loss: 0.4527\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4527)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:55<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 46/50 | Avg Loss: 0.4535\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:54<00:00,  5.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 47/50 | Avg Loss: 0.4484\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŒŸ New best model saved (Loss=0.4484)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:55<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 48/50 | Avg Loss: 0.4512\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:55<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 49/50 | Avg Loss: 0.4500\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1610/1610 [04:59<00:00,  5.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Epoch 50/50 | Avg Loss: 0.4485\n",
            "âœ… Saved checkpoint â†’ /content/models/latest_face_detector_multi_ratio.pth\n",
            "ðŸŽ¯ Training completed successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "#  Face Detector (Multi-Ratio Anchors, Focal Loss, Checkpointing)\n",
        "# ============================================================\n",
        "\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "import math, os\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = 300\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Model Definition\n",
        "# ------------------------------------------------------------\n",
        "class FaceDetector(nn.Module):\n",
        "    def __init__(self, num_anchor_scales=5, num_anchor_ratios=3):\n",
        "        super().__init__()\n",
        "        base_channels = 64\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(3, base_channels, 3, stride=2, padding=1),  # 150x150\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(base_channels, base_channels * 2, 3, stride=2, padding=1),  # 75x75\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(base_channels * 2, base_channels * 4, 3, stride=2, padding=1),  # 38x38\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.num_anchors = num_anchor_scales * num_anchor_ratios  # 5x3 = 15 anchors per cell\n",
        "\n",
        "        # prediction heads\n",
        "        self.cls_head = nn.Conv2d(base_channels * 4, self.num_anchors * 1, 3, padding=1)\n",
        "        self.reg_head = nn.Conv2d(base_channels * 4, self.num_anchors * 4, 3, padding=1)\n",
        "\n",
        "        for m in [self.cls_head, self.reg_head]:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            nn.init.constant_(m.bias, 0.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)                      # (B,256,38,38)\n",
        "        B = x.size(0)\n",
        "        A = self.num_anchors\n",
        "\n",
        "        cls_out = self.cls_head(x)                # (B,A,38,38)\n",
        "        reg_out = self.reg_head(x)                # (B,Ax4,38,38)\n",
        "\n",
        "        cls_out = cls_out.permute(0,2,3,1).contiguous().view(B, -1)\n",
        "        reg_out = reg_out.permute(0,2,3,1).contiguous().view(B, -1, 4)\n",
        "        return cls_out, reg_out\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Anchor Generator\n",
        "# ------------------------------------------------------------\n",
        "feature_H, feature_W = 38, 38\n",
        "stride = input_size / feature_H\n",
        "anchor_sizes = [16, 32, 64, 128, 256]\n",
        "ratios = [0.5, 1.0, 2.0]\n",
        "\n",
        "def make_anchors(feature_size, stride, sizes, ratios):\n",
        "    H, W = feature_size\n",
        "    anchors = []\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            cx, cy = (j + 0.5) * stride, (i + 0.5) * stride\n",
        "            for s in sizes:\n",
        "                for r in ratios:\n",
        "                    w = s * math.sqrt(r)\n",
        "                    h = s / math.sqrt(r)\n",
        "                    anchors.append([cx, cy, w, h])\n",
        "    return torch.tensor(anchors, dtype=torch.float32)\n",
        "\n",
        "anchors = make_anchors((feature_H, feature_W), stride, anchor_sizes, ratios).to(device)\n",
        "num_total_anchors = anchors.size(0)\n",
        "print(f\"âœ… Generated {num_total_anchors} anchors ({len(anchor_sizes)} scales Ã— {len(ratios)} ratios)\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# IoU Function\n",
        "# ------------------------------------------------------------\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
        "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    inter = wh[:,:,0] * wh[:,:,1]\n",
        "    iou = inter / (area1[:,None] + area2 - inter + 1e-8)\n",
        "    return iou\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Focal Loss\n",
        "# ------------------------------------------------------------\n",
        "def focal_loss(pred, target, alpha=0.25, gamma=2.0):\n",
        "    prob = torch.sigmoid(pred)\n",
        "    ce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
        "    p_t = prob * target + (1 - prob) * (1 - target)\n",
        "    loss = alpha * (1 - p_t) ** gamma * ce\n",
        "    return loss\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Initialize Model & Optimizer\n",
        "# ------------------------------------------------------------\n",
        "model = FaceDetector(num_anchor_scales=len(anchor_sizes), num_anchor_ratios=len(ratios)).to(device)\n",
        "print(f\"âœ… Model initialized: {model.num_anchors} anchors/cell Ã— {feature_H}Ã—{feature_W} = {model.num_anchors*feature_H*feature_W} total\")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
        "reg_loss_fn = nn.SmoothL1Loss(reduction='none')\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Checkpoint Setup\n",
        "# ------------------------------------------------------------\n",
        "save_dir = \"/content/models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(save_dir, \"latest_face_detector_multi_ratio.pth\")\n",
        "\n",
        "start_epoch = 0\n",
        "best_loss = float(\"inf\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"]\n",
        "    best_loss = checkpoint.get(\"best_loss\", best_loss)\n",
        "    print(f\"ðŸ” Resumed training from epoch {start_epoch} (best_loss={best_loss:.4f})\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Training Loop\n",
        "# ------------------------------------------------------------\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for imgs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        imgs = torch.stack(imgs).to(device)\n",
        "        batch_size = imgs.size(0)\n",
        "        cls_preds, reg_preds = model(imgs)\n",
        "\n",
        "        all_cls_loss, all_reg_loss, total_pos = 0.0, 0.0, 0\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            gt_boxes = targets[b]['boxes'].to(device)\n",
        "            if gt_boxes.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            anchor_boxes_xyxy = torch.cat([\n",
        "                anchors[:, :2] - anchors[:, 2:] / 2,\n",
        "                anchors[:, :2] + anchors[:, 2:] / 2\n",
        "            ], dim=1)\n",
        "            ious = box_iou(anchor_boxes_xyxy, gt_boxes)\n",
        "\n",
        "            best_anchor_idx = ious.argmax(dim=0)\n",
        "            max_iou_anchor, _ = ious.max(dim=1)\n",
        "\n",
        "            labels = torch.full((num_total_anchors,), -1.0, dtype=torch.float32, device=device)\n",
        "            labels[max_iou_anchor < 0.4] = 0.0\n",
        "            labels[best_anchor_idx] = 1.0\n",
        "            labels[max_iou_anchor >= 0.5] = 1.0\n",
        "\n",
        "            cls_pred = cls_preds[b].view(-1)\n",
        "            mask = (labels != -1)\n",
        "            if mask.sum() > 0:\n",
        "                cls_loss = focal_loss(cls_pred[mask], labels[mask]).sum()\n",
        "                all_cls_loss += cls_loss\n",
        "\n",
        "            pos_idx = torch.where(labels == 1)[0]\n",
        "            total_pos += pos_idx.numel()\n",
        "            if pos_idx.numel() > 0:\n",
        "                anchors_pos = anchors[pos_idx]\n",
        "                assigned_gt = gt_boxes[ious[pos_idx].argmax(dim=1)]\n",
        "                anc_cx, anc_cy, anc_w, anc_h = anchors_pos.t()\n",
        "                gt_cx = 0.5*(assigned_gt[:,0]+assigned_gt[:,2])\n",
        "                gt_cy = 0.5*(assigned_gt[:,1]+assigned_gt[:,3])\n",
        "                gt_w  = assigned_gt[:,2]-assigned_gt[:,0]\n",
        "                gt_h  = assigned_gt[:,3]-assigned_gt[:,1]\n",
        "                tx = (gt_cx - anc_cx) / anc_w\n",
        "                ty = (gt_cy - anc_cy) / anc_h\n",
        "                tw = torch.log(gt_w / anc_w + 1e-6)\n",
        "                th = torch.log(gt_h / anc_h + 1e-6)\n",
        "                reg_targets = torch.stack([tx, ty, tw, th], dim=1)\n",
        "                reg_pred = reg_preds[b][pos_idx]\n",
        "                reg_loss = reg_loss_fn(reg_pred, reg_targets).sum()\n",
        "                all_reg_loss += reg_loss\n",
        "\n",
        "        loss = (all_cls_loss + all_reg_loss) / max(total_pos, 1)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / max(len(train_loader), 1)\n",
        "    print(f\"ðŸ“˜ Epoch {epoch+1}/{num_epochs} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # ðŸ’¾ Save Checkpoints\n",
        "    torch.save({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"best_loss\": best_loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"âœ… Saved checkpoint â†’ {checkpoint_path}\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        best_path = os.path.join(save_dir, \"best_face_detector_multi_ratio.pth\")\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"ðŸŒŸ New best model saved (Loss={best_loss:.4f})\")\n",
        "\n",
        "print(\"ðŸŽ¯ Training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VG2sKo3GXVAG"
      },
      "outputs": [],
      "source": [
        "# Load model checkpoint\n",
        "# checkpoint = torch.load(\"/content/models/face_detector_epoch_10.pth\", map_location=device)\n",
        "\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# print(f\" Loaded model from epoch {start_epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SRUVjhFMKKGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e57545c-6981-47c3-d346-219bbb10c496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Precision: 0.461, Recall: 0.027\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_tp = all_fp = all_fn = 0\n",
        "    for imgs, targets in val_loader:\n",
        "        imgs = torch.stack(imgs).to(device)\n",
        "        cls_preds, reg_preds = model(imgs)\n",
        "        batch_size = imgs.size(0)\n",
        "        for b in range(batch_size):\n",
        "            # Ground truth boxes for this image\n",
        "            gt_boxes = targets[b]['boxes'].numpy()\n",
        "            # Decode predictions: apply sigmoid to scores\n",
        "            scores = torch.sigmoid(cls_preds[b]).cpu().numpy()\n",
        "            boxes = anchors.cpu().numpy()  # anchors [cx,cy,w,h]\n",
        "            # Decode predicted offsets\n",
        "            deltas = reg_preds[b].cpu().numpy()  # (A_total,4)\n",
        "            # Decode boxes: [cx,cy,w,h] + offsets -> [xmin,ymin,xmax,ymax]\n",
        "            cx = boxes[:,0] + deltas[:,0]*boxes[:,2]\n",
        "            cy = boxes[:,1] + deltas[:,1]*boxes[:,3]\n",
        "            w  = boxes[:,2] * np.exp(deltas[:,2])\n",
        "            h  = boxes[:,3] * np.exp(deltas[:,3])\n",
        "            # Convert to corners\n",
        "            pred_boxes = np.stack([cx - w/2, cy - h/2, cx + w/2, cy + h/2], axis=1)\n",
        "            # Filter by score threshold\n",
        "            threshold = 0.5\n",
        "            mask = scores > threshold\n",
        "            pred_boxes = pred_boxes[mask]\n",
        "            pred_scores = scores[mask]\n",
        "\n",
        "            # For simplicity, apply no NMS (assume faces are sparse).\n",
        "            # Otherwise run a greedy NMS to remove duplicates.\n",
        "\n",
        "            # Match predicted boxes to GT\n",
        "            if len(pred_boxes) == 0:\n",
        "                all_fn += gt_boxes.shape[0]\n",
        "                continue\n",
        "            if gt_boxes.shape[0] == 0:\n",
        "                all_fp += len(pred_boxes)\n",
        "                continue\n",
        "            # Compute IoU matrix\n",
        "            ious = box_iou(\n",
        "                torch.tensor(pred_boxes, dtype=torch.float32),\n",
        "                torch.tensor(gt_boxes, dtype=torch.float32)\n",
        "            ).numpy()\n",
        "            # Determine matches\n",
        "            assigned_gt = -np.ones(len(pred_boxes), dtype=int)\n",
        "            for i in range(len(pred_boxes)):\n",
        "                j = np.argmax(ious[i])\n",
        "                if ious[i,j] >= 0.5:\n",
        "                    assigned_gt[i] = j\n",
        "            # Count true positives (one per GT)\n",
        "            matched_gt = set()\n",
        "            tp = fp = fn = 0\n",
        "            for i, gt_idx in enumerate(assigned_gt):\n",
        "                if gt_idx >= 0 and gt_idx not in matched_gt:\n",
        "                    tp += 1\n",
        "                    matched_gt.add(gt_idx)\n",
        "                else:\n",
        "                    fp += 1\n",
        "            fn = gt_boxes.shape[0] - len(matched_gt)\n",
        "            all_tp += tp\n",
        "            all_fp += fp\n",
        "            all_fn += fn\n",
        "\n",
        "    precision = all_tp / (all_tp + all_fp + 1e-8)\n",
        "    recall    = all_tp / (all_tp + all_fn + 1e-8)\n",
        "    print(f\"Validation Precision: {precision:.3f}, Recall: {recall:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzaGs22TWOnh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "while True:\n",
        "    time.sleep(60)  # Waits for 60 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4JaEAbU4Ez3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ðŸŽ¨ Inference & Visualization for Face Detector\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Load Model + Anchors\n",
        "# ------------------------------------------------------------\n",
        "best_model_path = \"/content/models/best_face_detector_multi_ratio.pth\"\n",
        "\n",
        "model = FaceDetector(num_anchor_scales=len(anchor_sizes), num_anchor_ratios=len(ratios)).to(device)\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "model.eval()\n",
        "print(f\"Loaded model from: {best_model_path}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helper: Decode Anchors + Predictions\n",
        "# ------------------------------------------------------------\n",
        "def decode_predictions(cls_pred, reg_pred, anchors, conf_thresh=0.5):\n",
        "    \"\"\"\n",
        "    Decode model outputs into bounding boxes and confidences.\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(cls_pred)\n",
        "    mask = probs > conf_thresh\n",
        "    if mask.sum() == 0:\n",
        "        return []\n",
        "\n",
        "    probs = probs[mask]\n",
        "    boxes = anchors[mask]\n",
        "    reg = reg_pred[mask]\n",
        "\n",
        "    # Decode center/size offsets back to (xmin, ymin, xmax, ymax)\n",
        "    cx = boxes[:, 0] + reg[:, 0] * boxes[:, 2]\n",
        "    cy = boxes[:, 1] + reg[:, 1] * boxes[:, 3]\n",
        "    w  = boxes[:, 2] * torch.exp(reg[:, 2])\n",
        "    h  = boxes[:, 3] * torch.exp(reg[:, 3])\n",
        "\n",
        "    xmin = cx - w / 2\n",
        "    ymin = cy - h / 2\n",
        "    xmax = cx + w / 2\n",
        "    ymax = cy + h / 2\n",
        "\n",
        "    decoded = torch.stack([xmin, ymin, xmax, ymax, probs], dim=1)\n",
        "    return decoded.cpu().detach().numpy()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helper: Draw Boxes on Image\n",
        "# ------------------------------------------------------------\n",
        "def draw_boxes(image, gt_boxes, pred_boxes, show_gt=True):\n",
        "    img_np = np.array(image.copy())\n",
        "    img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Draw GT boxes in GREEN\n",
        "    if show_gt and gt_boxes is not None:\n",
        "        for (x1, y1, x2, y2) in gt_boxes:\n",
        "            cv2.rectangle(img_np, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "    # Draw predicted boxes in RED\n",
        "    for (x1, y1, x2, y2, conf) in pred_boxes:\n",
        "        cv2.rectangle(img_np, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)\n",
        "        cv2.putText(img_np, f\"{conf:.2f}\", (int(x1), int(y1)-4),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Test on a Few Validation Images\n",
        "# ------------------------------------------------------------\n",
        "num_samples = 5\n",
        "indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n",
        "\n",
        "for idx in indices:\n",
        "    img, target = val_dataset[idx]\n",
        "    img_in = img.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        cls_pred, reg_pred = model(img_in)\n",
        "\n",
        "    preds = decode_predictions(cls_pred[0], reg_pred[0], anchors, conf_thresh=0.4)\n",
        "\n",
        "    # Scale boxes back to image resolution (if needed)\n",
        "    gt_boxes = target['boxes'].cpu().numpy() if len(target['boxes']) > 0 else None\n",
        "\n",
        "    print(f\"Image {idx} â†’ Detected {len(preds)} faces\")\n",
        "    draw_boxes(transforms.ToPILImage()(img), gt_boxes, preds)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}